{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1M2CoZ34ZrKKplNTjMs9wazDFy_Scm6N1",
      "authorship_tag": "ABX9TyN1zLGxVFVm3mHgbTg4TwtI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhaw-iwi/RAG-week3/blob/main/RAGTweaking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Llama Index and RAG tweaking\n",
        "\n",
        "Tutorial heavily based on and adapted from: https://www.smashingmagazine.com/2024/01/guide-retrieval-augmented-generation-language-models/\n",
        "\n",
        "We are going to use a custom LM for the transformations below and the llama index framework: https://github.com/run-llama/llama_index\n",
        "\n",
        "## Setup\n",
        "\n",
        "- In case you do not have a google account, create one.\n",
        "- Set the huggingface_key and openai_key as secrets in colab\n",
        "- Change your runtime to a TPU instance (we GPU access to run this notebook, regular CPU instances are not enough)\n",
        "- Upload the example PDF to Google Drive\n"
      ],
      "metadata": {
        "id": "Pla3p_2mTcTK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qD_N8esGqvpm"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "userdata.get('huggingface_key')\n",
        "hf_token = userdata.get('huggingface_key')\n",
        "openai_key = userdata.get('openai_key')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup continued"
      ],
      "metadata": {
        "id": "ZPHss1_zWG77"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index transformers accelerate bitsandbytes\n",
        "!pip install chromadb sentence-transformers pydantic==1.10.11\n",
        "!pip install llama-index-vector-stores-chroma\n",
        "!pip install llama-index-embeddings-huggingface\n",
        "!pip install llama-index-llms-huggingface"
      ],
      "metadata": {
        "id": "jEZX-In0xeCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install accelerate"
      ],
      "metadata": {
        "id": "LAnrQ0C35tpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install -i https://pypi.org/simple/ bitsandbytes"
      ],
      "metadata": {
        "id": "AUYKls4050ma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports required for this notebook"
      ],
      "metadata": {
        "id": "909P5GqDX7yi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Import necessary libraries\n",
        "from llama_index.core import VectorStoreIndex, download_loader, ServiceContext\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "from llama_index.core.storage.storage_context import StorageContext\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core.response.notebook_utils import display_response\n",
        "import torch\n",
        "from transformers import BitsAndBytesConfig\n",
        "from llama_index.core.prompts import PromptTemplate\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from IPython.display import Markdown, display\n",
        "import chromadb\n",
        "from pathlib import Path\n",
        "import logging\n",
        "import sys\n",
        "from IPython.display import HTML, display\n",
        "from llama_index.core.indices.query.query_transform import HyDEQueryTransform\n",
        "from llama_index.core.query_engine.transform_query_engine import (\n",
        "    TransformQueryEngine,\n",
        ")\n",
        "from llama_index.core.indices.document_summary import DocumentSummaryIndex"
      ],
      "metadata": {
        "id": "25a4K70yxqD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "PDFReader = download_loader(\"PDFReader\")\n",
        "loader = PDFReader()\n",
        "documents = loader.load_data(file=Path('drive/MyDrive/ARM-RAG.pdf'))"
      ],
      "metadata": {
        "id": "5L-3wvAvyk33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Compression\n",
        "\n",
        "The Language Model, in this case `Llama-2-7b-chat` is about 7GB in size and it is not even the biggest in the family. To process it more efficiently on a single TPU, we have to compress it further.\n",
        "Normally, a language model uses 32-bit floating-point numbers to represent the weights in its neural network. In 4-bit quantization, these weights are converted into 4-bit representations, which are much smaller in size. While this can lead to some loss of information or precision, careful design and training techniques can minimize these effects. The result is a more compact model that requires less memory and computational power, making it more practical for use in real-world applications, particularly on mobile devices or other hardware with limited processing capabilities.\n",
        "\n",
        "https://huggingface.co/blog/4bit-transformers-bitsandbytes"
      ],
      "metadata": {
        "id": "wQq60TlO5TSi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# config for the quantization, applied when loading the model below.\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "  load_in_4bit=True,\n",
        "  bnb_4bit_compute_dtype=torch.float16,\n",
        "  bnb_4bit_quant_type=\"nf4\",\n",
        "  bnb_4bit_use_double_quant=True,\n",
        ")"
      ],
      "metadata": {
        "id": "z9gW6Hbr3hxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attention\n",
        "**The following just works, if you start correctly started Colab as GPU/TPU instance.**\n",
        "\n",
        "## Model Selection\n",
        "While we use LLama2-chat here you can also pick another model from the Huggingface hub: https://huggingface.co/models?pipeline_tag=text-generation&sort=trending"
      ],
      "metadata": {
        "id": "9T-KF3WN-bEG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = HuggingFaceLLM(\n",
        "    model_name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
        "    tokenizer_name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
        "    query_wrapper_prompt=PromptTemplate(\"<s> [INST] {query_str} [/INST] \"),\n",
        "    context_window=3900,\n",
        "    model_kwargs={\"token\": hf_token, \"quantization_config\": quantization_config},\n",
        "    tokenizer_kwargs={\"token\": hf_token},\n",
        "    device_map=\"auto\",\n",
        ")"
      ],
      "metadata": {
        "id": "YzqBRWQk5G0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test the model with a simple completion"
      ],
      "metadata": {
        "id": "AHZl928ib2Hc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Assuming resp contains the response\n",
        "resp = llm.complete(\"What is ARM-RAG?\")\n",
        "\n",
        "# Using HTML with inline CSS for styling (blue color, smaller font size)\n",
        "html_text = f'<p style=\"color: #1f77b4; font-size: 14px;\"><b>{resp}</b></p>'"
      ],
      "metadata": {
        "id": "9LyoFr-y5nDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(HTML(html_text))\n"
      ],
      "metadata": {
        "id": "QL4WA0TYFEBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Chroma Collection setup"
      ],
      "metadata": {
        "id": "bFWLzpwpb6_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create client and a new collection\n",
        "chroma_client = chromadb.EphemeralClient()\n",
        "chroma_collection = chroma_client.create_collection(\"firstcollection\")"
      ],
      "metadata": {
        "id": "26J3hGDoFOzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Select embedding model\n",
        "Same as last week: we have different options for embedding models: https://huggingface.co/spaces/mteb/leaderboard\n",
        "\n",
        "This time we can use one that is higher up on the leaderboard. We are no longer restricted by our laptops' hardware ðŸ˜œ"
      ],
      "metadata": {
        "id": "20NDzOGmGYTx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n"
      ],
      "metadata": {
        "id": "RmAX97trFwp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load data into vector store"
      ],
      "metadata": {
        "id": "VMvKzqVhcz4w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set up ChromaVectorStore and load in data\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model)\n",
        "index = VectorStoreIndex.from_documents(\n",
        "  documents, storage_context=storage_context, service_context=service_context)"
      ],
      "metadata": {
        "id": "gZ3USPSfF1Of"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup summarizer for retrieved Documents\n",
        "\n",
        "\n",
        "We should also establish a way for the model to summarize the data rather than spitting everything out at once. A SummaryIndex offers efficient summarization and retrieval of information:"
      ],
      "metadata": {
        "id": "daxsxpbsH-Po"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summary_index = DocumentSummaryIndex.from_documents(documents, service_context=service_context)\n"
      ],
      "metadata": {
        "id": "ovjx1H6AGwgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can test the same query with our vector store"
      ],
      "metadata": {
        "id": "wXHHDoller5r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define your query\n",
        "query=\"what is ARM-RAG?\"\n",
        "\n",
        "#from llama_index.core.embeddings import similarity\n",
        "query_engine =index.as_query_engine(response_mode=\"compact\")\n",
        "response = query_engine.query(query)\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "# Using HTML with inline CSS for styling (blue color)\n",
        "html_text = f'<p style=\"color: #1f77b4; font-size: 14px;\"><b>{response}</b></p>'\n",
        "display(HTML(html_text))"
      ],
      "metadata": {
        "id": "WrWGDd8rG7vX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### More Complex Request\n",
        "\n",
        "**Key Learning:** Llama_index uses different retrievers for different application types.\n"
      ],
      "metadata": {
        "id": "2v6BKCEPfDmj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_engine = index.as_chat_engine(chat_mode=\"condense_question\", verbose=True)\n",
        "response = chat_engine.chat(\"give me real world examples of apps/system i can build leveraging ARM-RAG?\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "KREt5zQZIlUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Index as simple retriever\n",
        "\n",
        "With output of the retrieved documents..."
      ],
      "metadata": {
        "id": "VNWnwHkogFBu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = index.as_retriever() #similarity_top_k=3\n",
        "retrieval_results = retriever.retrieve(\"what is ARM-RAG?\")\n",
        "for i, res in enumerate(retrieval_results):\n",
        "  print(i, \"\\n\", res.node.get_text())"
      ],
      "metadata": {
        "id": "_qbah69WIxEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HyDE with LLama_Index\n",
        "\n",
        "The important part is: Even if you cannot make it run with llama_index, once you know how it works, you can implement it yourself."
      ],
      "metadata": {
        "id": "0fgiLd5LjUSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "query_engine = index.as_query_engine(similarity_top_k=3)\n"
      ],
      "metadata": {
        "id": "f3jbsolyMsAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hyde = HyDEQueryTransform(include_original=True, llm=llm)\n",
        "hyde_query_engine = TransformQueryEngine(query_engine, hyde)"
      ],
      "metadata": {
        "id": "Srfu4knpQHx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = hyde_query_engine.query(\"what is ARM-RAG??\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "1HMHGveEQRQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "hypothetical answer generated by hyde"
      ],
      "metadata": {
        "id": "sSMBzG3vVrYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_bundle = hyde(\"what is ARM-RAG?\")\n",
        "hyde_doc = query_bundle.embedding_strs[0]\n",
        "hyde_doc"
      ],
      "metadata": {
        "id": "9dPFTIyRVX3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setup base query engine as tool\n",
        "from llama_index.core.tools import QueryEngineTool\n",
        "from llama_index.core.tools import ToolMetadata\n",
        "from llama_index.core.query_engine import SubQuestionQueryEngine\n",
        "\n",
        "query_engine_tools = [\n",
        "    QueryEngineTool(\n",
        "        query_engine=query_engine,\n",
        "        metadata=ToolMetadata(\n",
        "            name=\"pg_essay\",\n",
        "            description=\"Paul Graham essay on What I Worked On\",\n",
        "        ),\n",
        "    ),\n",
        "]\n",
        "query_engine = SubQuestionQueryEngine.from_defaults(\n",
        "    query_engine_tools=query_engine_tools,\n",
        "    use_async=True,\n",
        ")"
      ],
      "metadata": {
        "id": "H6JSpIJgQq8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "%pip install llama-index-llms-openai"
      ],
      "metadata": {
        "id": "xDaYq1sxlvaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from llama_index.llms import OpenAI\n",
        "os.environ['OPENAI_API_KEY'] = openai_key\n",
        "llm_predictor_= LLMPredictor(llm=ChatOpenAI(temperature=0.0, model_name=\"gpt-4\" , max_tokens=4096, request_timeout=120))\n",
        "new_service_context_ = ServiceContext.from_defaults(llm_predictor=llm_predictor)"
      ],
      "metadata": {
        "id": "G5Id_GWwS5I-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Query Augmentation and Cross-Encoder Reraking\n",
        "\n",
        "DIY- Method"
      ],
      "metadata": {
        "id": "Lz8E42felf3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n",
        "#!pip install chroma"
      ],
      "metadata": {
        "id": "B88f_Rm6l6PF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pypdf import PdfReader\n",
        "\n",
        "reader = PdfReader(file=Path('drive/MyDrive/ARM-RAG.pdf'))\n",
        "pdf_texts = [p.extract_text().strip() for p in reader.pages]\n",
        "\n",
        "# Filter the empty strings\n",
        "pdf_texts = [text for text in pdf_texts if text]\n",
        "\n",
        "print(pdf_texts[0])"
      ],
      "metadata": {
        "id": "RwxemLkgZ7kM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, SentenceTransformersTokenTextSplitter\n",
        "character_splitter = RecursiveCharacterTextSplitter(\n",
        "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=0\n",
        ")\n",
        "character_split_texts = character_splitter.split_text('\\n\\n'.join(pdf_texts))\n",
        "print(f\"\\nTotal chunks: {len(character_split_texts)}\")"
      ],
      "metadata": {
        "id": "VqEeG7Nwl2mF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_splitter = SentenceTransformersTokenTextSplitter(chunk_overlap=0, tokens_per_chunk=128, model_name=\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
        "\n",
        "token_split_texts = []\n",
        "for text in character_split_texts:\n",
        "    token_split_texts += token_splitter.split_text(text)\n",
        "\n",
        "print(f\"\\nTotal chunks: {len(token_split_texts)}\")"
      ],
      "metadata": {
        "id": "nuSCOfEgl_07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb\n",
        "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n",
        "\n",
        "embedding_function = SentenceTransformerEmbeddingFunction(model_name=\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
        "print(embedding_function([token_split_texts[10]])"
      ],
      "metadata": {
        "id": "4BlmrdWCmLrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "id": "9TQKaVI0m38o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def augment_multiple_query(query):\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"invent a prompt to generate more queries\n",
        "            \"Gib eine Frage pro Zeile und ohne Nummerierung aus.\"\n",
        "        },\n",
        "        {\"role\": \"user\", \"content\": query}\n",
        "    ]\n",
        "\n",
        "    client = OpenAI()\n",
        "    response = client.chat.completions.create(\n",
        "          messages=messages,\n",
        "          model=DEFINE_MODEL\n",
        "    )\n",
        "    content = response.choices[0].message.content\n",
        "    content = content.split(\"\\n\")\n",
        "    return content"
      ],
      "metadata": {
        "id": "h6p-I4vymRgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_query = \"\"\n",
        "augmented_queries = augment_multiple_query(original_query)\n",
        "\n",
        "augmented_query_list = []\n",
        "for query in augmented_queries:\n",
        "    augmented_query_list.append(query)\n",
        "    print(query)\n",
        "\n",
        "print(augmented_query_list)"
      ],
      "metadata": {
        "id": "rrLosluomZSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "queries = [original_query] + augmented_queries\n",
        "results = chroma_collection.query(query_texts=queries, n_results=5, include=['documents', 'embeddings'])\n",
        "\n",
        "retrieved_documents = results['documents']\n",
        "\n",
        "# Deduplicate the retrieved documents\n",
        "unique_documents = set()\n",
        "for documents in retrieved_documents:\n",
        "    for document in documents:\n",
        "        unique_documents.add(document)\n",
        "\n",
        "for i, documents in enumerate(retrieved_documents):\n",
        "    print(f\"Query: {queries[i]}\")\n",
        "    print('')\n",
        "    print(\"Results:\")\n",
        "    for doc in documents:\n",
        "        print(word_wrap(doc))\n",
        "        print('')\n",
        "    print('-'*100)"
      ],
      "metadata": {
        "id": "S1u1pyoAnyit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reranking"
      ],
      "metadata": {
        "id": "09zvERr8n7-0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "original_query = \"\"\n",
        "generated_queries = augmented_query_list"
      ],
      "metadata": {
        "id": "Qs5uXr_Yn4N4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pairs = []\n",
        "for doc in unique_documents:\n",
        "    pairs.append([original_query, doc])"
      ],
      "metadata": {
        "id": "khxhaB7hoLwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import CrossEncoder\n",
        "cross_encoder = CrossEncoder('cross-encoder/msmarco-MiniLM-L6-en-de-v1')"
      ],
      "metadata": {
        "id": "9ClxNk6goOAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = cross_encoder.predict(pairs)"
      ],
      "metadata": {
        "id": "n8iwYnc-oPvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Scores:\")\n",
        "for score in scores:\n",
        "    print(score)"
      ],
      "metadata": {
        "id": "RnwCNzknoSBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# select top 3 documents\n",
        "new_order = np.argsort(scores)[::-1]\n",
        "top_3 = new_order[:3]\n",
        "print(\"New Ordering top 3:\")\n",
        "for o in top_3:\n",
        "    print(o)"
      ],
      "metadata": {
        "id": "objmxZTVoT41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_answer(query, retrieved_documents, top_3):\n",
        "\n",
        "    # retrieve pick most relevant documents by index @ToDo: refactor into separate function.\n",
        "    most_relevant_docs = []\n",
        "    for i in top_3:\n",
        "        most_relevant_docs.append(retrieved_documents[i])\n",
        "\n",
        "    information = \"\\n\\n\".join(most_relevant_docs)\n",
        "    #print(word_wrap(information))\n",
        "\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": f\"\"\"\n",
        "            prompt\n",
        "            \"\"\"\n",
        "    },\n",
        "        {\"role\": \"user\", \"content\": f\"Informationen: {information}\"}\n",
        "    ]\n",
        "    #print(messages)\n",
        "    client = OpenAI()\n",
        "    response = client.chat.completions.create(\n",
        "          messages=messages,\n",
        "          model=DEFINE_MODEL\n",
        "    )\n",
        "    content = response.choices[0].message.content\n",
        "    content = response.choices[0].message.content\n",
        "    return content"
      ],
      "metadata": {
        "id": "APSIobYsoamN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}